{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114f4a8-9a73-4254-92ce-09c3e1688c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cftime\n",
    "import h5py\n",
    "import xesmf as xe   # ya no se usa, pero puedes dejarlo si lo necesitas más adelante\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from pyproj import Geod\n",
    "from matplotlib.colors import LogNorm\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import json\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", module=\"xarray.coding.times\")\n",
    "warnings.filterwarnings(\"ignore\", module=\"xarray.coding.cftime_offsets\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"xarray\")\n",
    "warnings.filterwarnings(\"ignore\", module=\"xarray\")\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "# CONFIGURACIÓN GENERAL DE RUTAS\n",
    "# Define el escenario\n",
    "escenario = \"SSP585\"  # Cambia a \"SSP585\" cuando lo necesites\n",
    "\n",
    "# Carpeta general donde guardar los resultados\n",
    "BASE_RESULTADOS = \"C:/Users/jaime/Desktop/Universidad/TFG/Resultados\"\n",
    "\n",
    "# Subcarpeta específica del escenario (para gráficos, mapas, etc.)\n",
    "base_out = os.path.join(BASE_RESULTADOS, escenario, \"CAT index\")\n",
    "os.makedirs(base_out, exist_ok=True)\n",
    "\n",
    "# Archivo común de umbrales (solo se calcula una vez desde SSP245)\n",
    "UMBRAL_FILE = os.path.join(BASE_RESULTADOS, \"umbrales_fijos_desde_ssp245.json\")\n",
    "\n",
    "base_out2 = f\"C:/Users/jaime/Desktop/Universidad/TFG/Resultados/{escenario}/zonal wind\"\n",
    "os.makedirs(base_out2, exist_ok=True)\n",
    "\n",
    "# Lista de modelos a analizar\n",
    "modelos = [\n",
    "    \"IPSL-CM6A-LR\", \"HadGEM3-GC31-LL\", \"CESM2-WACCM\", \"GFDL-CM4\",\n",
    "    \"MPI-ESM1-2-LR\", \"NorESM2-LM\", \"ACCESS-CM2\",\n",
    "    \"EC-Earth3-Veg-LR\", \"MIROC6\", \"TaiESM1\"\n",
    "]\n",
    "\n",
    "# CARGAR VARIABLES PROCESADAS DESDE LOS .NC\n",
    "# Ruta donde ya tienes las variables procesadas y guardadas (ua, va, zg, ta)\n",
    "ruta_guardado = f\"D:/TFG/Variables tratadas/{escenario}\"\n",
    "\n",
    "# FUNCIONES ADICIONALES\n",
    "def asignar_invierno(ds):\n",
    "    \"\"\"\n",
    "    Asigna cada fecha DJF al 'winter_year' correspondiente\n",
    "    (p. ej., DJF 2040-2041 → invierno 2041)\n",
    "    \"\"\"\n",
    "    meses = ds['time'].dt.month\n",
    "    años = ds['time'].dt.year\n",
    "    winter_year = xr.where(meses == 12, años + 1, años)\n",
    "    return ds.assign_coords(winter_year=(\"time\", winter_year.data))\n",
    "\n",
    "\n",
    "# FUNCIONES PARA DEF (para TI1)\n",
    "def _metric_derivs(u, v):\n",
    "    \"\"\"\n",
    "    Calcula derivadas horizontales de u y v sobre una esfera (m/s por metro)\n",
    "    para estimar el tensor de deformación (stretching y shearing).\n",
    "    \"\"\"\n",
    "    R = 6_371_000.0\n",
    "    deg2rad = np.pi / 180.0\n",
    "    rad2deg = 180.0 / np.pi\n",
    "\n",
    "    lat_rad = np.deg2rad(u['lat'])\n",
    "    coslat  = np.cos(lat_rad)\n",
    "\n",
    "    # Derivadas por grado (lo hace xarray)\n",
    "    dudlon_deg = u.differentiate('lon')   # du/dλ (por grado)\n",
    "    dudlat_deg = u.differentiate('lat')   # du/dφ (por grado)\n",
    "    dvdlon_deg = v.differentiate('lon')\n",
    "    dvdlat_deg = v.differentiate('lat')\n",
    "\n",
    "    # Convertir \"por grado\" -> \"por radian\"\n",
    "    ## Son derivadas por lo que tengo m/s/grados y quiero (m/s)/rad; por eso se multiplica por 180/pi\n",
    "    dudlon_rad = dudlon_deg * rad2deg\n",
    "    dudlat_rad = dudlat_deg * rad2deg\n",
    "    dvdlon_rad = dvdlon_deg * rad2deg\n",
    "    dvdlat_rad = dvdlat_deg * rad2deg\n",
    "\n",
    "    # Pasar a derivadas espaciales (por metro)\n",
    "    # ∂/∂x = (1/(R cosφ)) ∂/∂λ(rad)\n",
    "    # ∂/∂y = (1/R) ∂/∂φ(rad)\n",
    "    dudx = dudlon_rad / (R * coslat)\n",
    "    dudy = dudlat_rad / R\n",
    "    dvdx = dvdlon_rad / (R * coslat)\n",
    "    dvdy = dvdlat_rad / R\n",
    "\n",
    "    return dudx, dudy, dvdx, dvdy\n",
    "\n",
    "\n",
    "def compute_def(u, v):\n",
    "    \"\"\"\n",
    "    Calcula el módulo de deformación:\n",
    "    √[(∂u/∂x - ∂v/∂y)² + (∂v/∂x + ∂u/∂y)²]\n",
    "    \"\"\"\n",
    "    dudx, dudy, dvdx, dvdy = _metric_derivs(u, v)\n",
    "    stretching = dudx - dvdy\n",
    "    shearing   = dvdx + dudy\n",
    "    return np.sqrt(stretching**2 + shearing**2)\n",
    "\n",
    "\n",
    "# FUNCIONES PARA EL NÚMERO DE RICHARDSON GRADIENTE\n",
    "\n",
    "def compute_theta(T, p):\n",
    "    \"\"\"Temperatura potencial θ [K]\"\"\"\n",
    "    R_cp = 0.2856  # Aproximación R/Cp\n",
    "    p0 = 100000.0  # Pa\n",
    "    return T * (p0 / p)**R_cp\n",
    "\n",
    "\n",
    "def compute_N2(theta, zg):\n",
    "    \"\"\"Frecuencia de Brunt–Väisälä al cuadrado (N²)\"\"\"\n",
    "    g = 9.80665\n",
    "    dtheta_dplev = theta.differentiate(\"plev\")\n",
    "    dz_dplev = zg.differentiate(\"plev\")\n",
    "    dtheta_dz = dtheta_dplev / dz_dplev\n",
    "    N2 = (g / theta) * dtheta_dz\n",
    "    # Quita infinitos/nans raros\n",
    "    return N2.where(np.isfinite(N2))\n",
    "\n",
    "\n",
    "def compute_shear2(u, v, zg):\n",
    "    \"\"\"Magnitud del shear vertical²\"\"\"\n",
    "    dudz = u.differentiate(\"plev\") / zg.differentiate(\"plev\")\n",
    "    dvdz = v.differentiate(\"plev\") / zg.differentiate(\"plev\")\n",
    "    shear2 = dudz**2 + dvdz**2\n",
    "    # Evita valores ~0 que disparan Ri\n",
    "    return shear2.where(shear2 > 1e-8)  # s^-2 (umbral conservador)\n",
    "\n",
    "\n",
    "def compute_Ri(u, v, T, zg):\n",
    "    \"\"\"\n",
    "    Calcula el número de Richardson Ri = N² / (∂V/∂z)²,\n",
    "    filtrando solo zonas de estratificación estable (N² > 0)\n",
    "    \"\"\"\n",
    "    theta = compute_theta(T, u[\"plev\"])\n",
    "    N2 = compute_N2(theta, zg)\n",
    "    shear2 = compute_shear2(u, v, zg)\n",
    "    N2 = N2.where(N2 > 0)\n",
    "    Ri = N2 / shear2\n",
    "    return Ri\n",
    "\n",
    "# EJEMPLO RUTA TRANSATLÁNTICA\n",
    "lon_DUB, lat_DUB = -6.2621, 53.4287   # Dublín\n",
    "lon_JFK, lat_JFK = -73.7781, 40.6413  # Nueva York JFK\n",
    "geod = Geod(ellps=\"WGS84\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d70f6-7040-4031-adb2-02d07343d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REINICIO DEL SCRIPT DESDE VARIABLES YA PROCESADAS\n",
    "\n",
    "ua_models, va_models, zg_models, ta_models = [], [], [], []\n",
    "shear_slopes = []\n",
    "VWS_mean_models = []\n",
    "ti1_slopes = []\n",
    "ti1_mean_models = []\n",
    "ti1_signif_models = []\n",
    "ti1_daily_all_models = []\n",
    "VWS_daily_all_models = []\n",
    "Ri_slopes = []\n",
    "\n",
    "for modelo in modelos:\n",
    "    ruta_modelo = os.path.join(ruta_guardado, modelo)\n",
    "    if not os.path.isdir(ruta_modelo):\n",
    "        print(f\" {modelo}: carpeta no encontrada ({ruta_modelo})\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        ua = xr.open_dataset(os.path.join(ruta_modelo, f\"{modelo}_ua_DJF.nc\"))[\"ua\"]\n",
    "        va = xr.open_dataset(os.path.join(ruta_modelo, f\"{modelo}_va_DJF.nc\"))[\"va\"]\n",
    "        zg = xr.open_dataset(os.path.join(ruta_modelo, f\"{modelo}_zg_DJF.nc\"))[\"zg\"]\n",
    "        ta = xr.open_dataset(os.path.join(ruta_modelo, f\"{modelo}_ta_DJF.nc\"))[\"ta\"]\n",
    "    except Exception as e:\n",
    "        print(f\"{modelo}: error al cargar archivos procesados ({e})\")\n",
    "        continue\n",
    "\n",
    "    # Añadir coordenada 'modelo' para mantener compatibilidad\n",
    "    ua = ua.expand_dims(modelo=[modelo])\n",
    "    va = va.expand_dims(modelo=[modelo])\n",
    "    zg = zg.expand_dims(modelo=[modelo])\n",
    "    ta = ta.expand_dims(modelo=[modelo])\n",
    "\n",
    "    ua_models.append(ua)\n",
    "    va_models.append(va)\n",
    "    zg_models.append(zg)\n",
    "    ta_models.append(ta)\n",
    "\n",
    "    print(f\"{modelo}: variables procesadas cargadas correctamente.\")\n",
    "\n",
    "    print(\"\\n Todas las variables procesadas han sido cargadas.\")\n",
    "    print(f\"Modelos cargados: {[m.modelo.values.item() for m in ua_models]}\")\n",
    "\n",
    "    # CÁLCULOS DE VWS USANDO zg\n",
    "    u250 = ua.sel(plev=25000, method='nearest')\n",
    "    u500 = ua.sel(plev=50000, method='nearest')\n",
    "    v250 = va.sel(plev=25000, method='nearest')\n",
    "    v500 = va.sel(plev=50000, method='nearest')\n",
    "    dz = zg.sel(plev=25000, method='nearest') - zg.sel(plev=50000, method='nearest')\n",
    "    if zg.plev.values[0] < zg.plev.values[-1]:\n",
    "        dz = -dz  # corrige el signo si necesario\n",
    "\n",
    "    # Evitar divisiones por cero o por capas demasiado delgadas\n",
    "    dz = dz.where((dz > 100) & (dz < 10000))\n",
    "    VWS = np.sqrt((u250 - u500)**2 + (v250 - v500)**2) / dz\n",
    "\n",
    "    # GUARDAR VWS DIARIO DJF (para percentiles multimodelo)\n",
    "    VWS = VWS.where(np.isfinite(VWS))  # elimina inf o valores absurdos\n",
    "    VWS = VWS.rename(\"VWS\")\n",
    "    VWS_daily_all_models.append(VWS) \n",
    "\n",
    "    VWS_with_year = asignar_invierno(VWS)\n",
    "    VWS_djf_yearly = VWS_with_year.groupby(\"winter_year\").mean(\"time\")\n",
    "\n",
    "    # Media climatológica DJF del TI1 (2030–2100)\n",
    "    VWS_mean = VWS_djf_yearly.mean(\"winter_year\")\n",
    "    VWS_mean = VWS_mean.rename(\"VWS_mean\")\n",
    "\n",
    "    # Guardar para multimodelo\n",
    "    # VWS_mean = VWS_mean.assign_coords(modelo=modelo)\n",
    "    VWS_mean_models.append(VWS_mean)\n",
    "\n",
    "    ######################################################################################\n",
    "    VWS_trend = VWS_djf_yearly.polyfit(dim=\"winter_year\", deg=1)\n",
    "    VWS_slope = VWS_trend[\"polyfit_coefficients\"].sel(degree=1) * 10\n",
    "    # VWS_slope = VWS_slope.assign_coords(modelo=modelo)\n",
    "    shear_slopes.append(VWS_slope)\n",
    "\n",
    "    print(f\"\\n {modelo}: diagnóstico VWS/TI1\")\n",
    "    print(f\" - zg units: {zg.attrs.get('units', 'sin unidades')}\")\n",
    "    print(f\" - plev: {ua.plev.values}\")\n",
    "    print(f\" - zg500 media: {float(zg.sel(plev=50000, method='nearest').mean())}\")\n",
    "    print(f\" - zg250 media: {float(zg.sel(plev=25000, method='nearest').mean())}\")\n",
    "    print(f\" - Δz medio (m): {float((zg.sel(plev=25000, method='nearest') - zg.sel(plev=50000, method='nearest')).mean())}\")\n",
    "    print(f\" - |u250-u500| media: {float(np.abs((u250 - u500).mean()))}\")\n",
    "    print(f\" - |v250-v500| media: {float(np.abs((v250 - v500).mean()))}\")\n",
    "    print(f\" - VWS min/max: {float(VWS.min())}/{float(VWS.max())}\")\n",
    "\n",
    "    # Detectar orden de niveles\n",
    "    plev_vals = ua.plev.values\n",
    "    ascendente = plev_vals[0] < plev_vals[-1]\n",
    "\n",
    "    # Definir rango de presiones (Pa) más amplio y con sentido correcto\n",
    "    if ascendente:\n",
    "        plev_slice = slice(20000, 60000)\n",
    "    else:\n",
    "        plev_slice = slice(60000, 20000)\n",
    "\n",
    "    # Seleccionar capa\n",
    "    u_layer = ua.sel(plev=plev_slice)\n",
    "    v_layer = va.sel(plev=plev_slice)\n",
    "    zg_layer = zg.sel(plev=plev_slice)\n",
    "    ta_layer = ta.sel(plev=plev_slice)\n",
    "\n",
    "    print(f\"{modelo}: niveles seleccionados -> {u_layer.plev.values}\")\n",
    "\n",
    "    if u_layer.plev.size < 2:\n",
    "        print(f\" {modelo}: solo {u_layer.plev.size} niveles dentro del rango {plev_slice}, se omite TI1\")\n",
    "        continue\n",
    "\n",
    "    u_layer = ua.sel(plev=plev_slice)\n",
    "    v_layer = va.sel(plev=plev_slice)\n",
    "    zg_layer = zg.sel(plev=plev_slice)\n",
    "    ta_layer = ta.sel(plev=plev_slice)\n",
    "\n",
    "    # Calcular el término de deformación (shear tensor)\n",
    "    DEF_layer = compute_def(u_layer, v_layer)\n",
    "\n",
    "    p = u_layer[\"plev\"]\n",
    "    p_vals = p.values.astype(float)\n",
    "    dp = np.gradient(p_vals)\n",
    "    w = xr.DataArray(np.abs(dp), dims=[\"plev\"], coords={\"plev\": p})\n",
    "\n",
    "    DEF_layer = DEF_layer.where(np.isfinite(DEF_layer))\n",
    "    DEF_bar = (DEF_layer * w).sum(\"plev\") / w.sum(\"plev\")\n",
    "\n",
    "    # AÑADE AQUÍ LOS PRINTS DE DIAGNÓSTICO\n",
    "    print(f\"{modelo}: VWS min/max = {float(VWS.min())}/{float(VWS.max())}\")\n",
    "    print(f\"{modelo}: DEF_bar min/max = {float(DEF_bar.min())}/{float(DEF_bar.max())}\")\n",
    "\n",
    "    VWS = VWS.where(np.isfinite(VWS))\n",
    "    DEF_bar = DEF_bar.where(np.isfinite(DEF_bar))\n",
    "    DEF_bar = DEF_bar.clip(min=0)\n",
    "\n",
    "    TI1 = (VWS * DEF_bar)\n",
    "    TI1.attrs[\"units\"] = \"s⁻²\"\n",
    "    # TI1 = TI1.clip(min=0, max=20e-7)\n",
    "    TI1 = TI1.rename(\"TI1 (s^-2)\")\n",
    "\n",
    "    print(f\"{modelo}: TI1 min/max = {float(TI1.min())}/{float(TI1.max())}\")\n",
    "\n",
    "    ti1_min = float(TI1.min())\n",
    "    ti1_max = float(TI1.max())\n",
    "    if not np.isfinite(ti1_min) or not np.isfinite(ti1_max) or ti1_max == 0:\n",
    "        vabs = 1e-6\n",
    "    else:\n",
    "        vabs = max(abs(ti1_min), abs(ti1_max))\n",
    "    vabs = max(vabs, 1e-7)\n",
    "    norm = mcolors.TwoSlopeNorm(vmin=-vabs, vcenter=0, vmax=vabs)\n",
    "\n",
    "    TI1_with_year = asignar_invierno(TI1)\n",
    "\n",
    "    # GUARDAR VALORES DIARIOS DJF PARA PERCENTILES\n",
    "    ti1_daily_all_models.append(TI1_with_year)\n",
    "    TI1_djf_yearly = TI1_with_year.groupby(\"winter_year\").mean(\"time\")\n",
    "\n",
    "    # Media climatológica DJF del TI1 (2030–2100)\n",
    "    TI1_mean = TI1_djf_yearly.mean(\"winter_year\")\n",
    "    TI1_mean = TI1_mean.rename(\"TI1_mean\")\n",
    "\n",
    "    # TI1_mean = TI1_mean.assign_coords(modelo=modelo)\n",
    "    ti1_mean_models.append(TI1_mean)\n",
    "\n",
    "    #######################################################################################\n",
    "    TI1_trend = TI1_djf_yearly.polyfit(dim=\"winter_year\", deg=1)\n",
    "    TI1_slope = TI1_trend[\"polyfit_coefficients\"].sel(degree=1) * 10\n",
    "    # TI1_slope = TI1_slope.assign_coords(modelo=modelo)\n",
    "    ti1_slopes.append(TI1_slope)\n",
    "\n",
    "# CÁLCULO DEL PORCENTAJE DE DÍAS CON TI₁ > 12×10⁻⁷\n",
    "\n",
    "UMBRAL_TI1 = 2e-7  # umbral del paper (Jaeger 2007, Ellrod Knapp)\n",
    "\n",
    "ti1_freq_models = []  # aquí guardaremos las frecuencias (%)\n",
    "\n",
    "for modelo, TI1 in zip(modelos, ti1_daily_all_models):\n",
    "\n",
    "    print(f\"\\n Calculando frecuencia TI₁ > {UMBRAL_TI1:.1e} para {modelo}...\")\n",
    "\n",
    "    # TI₁ ya tiene coordenadas (time, lat, lon[, modelo])\n",
    "    # Crear máscara booleana (True si TI₁ > umbral)\n",
    "    exceed = TI1 > UMBRAL_TI1\n",
    "\n",
    "    # Calcular el porcentaje de días (respecto al total de días válidos)\n",
    "    #    (equivalente a 100 * sum / count, pero más robusto frente a NaNs)\n",
    "    freq = exceed.mean(dim=\"time\", skipna=True) * 100\n",
    "\n",
    "    # Añadir dimensión 'modelo' (solo si no existe)\n",
    "    if \"modelo\" not in freq.dims:\n",
    "        freq = freq.expand_dims(modelo=[modelo])\n",
    "    else:\n",
    "        freq = freq.assign_coords(modelo=[modelo])\n",
    "\n",
    "    freq.name = \"TI1_freq\"\n",
    "    freq.attrs[\"units\"] = \"%\"\n",
    "    freq.attrs[\"description\"] = f\"Porcentaje de días (2030–2100) con TI₁ > {UMBRAL_TI1:.1e} s⁻²\"\n",
    "\n",
    "    # diagnóstico rápido\n",
    "    print(f\"   Máx frecuencia para {modelo}: {float(freq.max()):.3f} %\")\n",
    "\n",
    "    ti1_freq_models.append(freq)\n",
    "\n",
    "# CÁLCULO DEL PORCENTAJE DE DÍAS CON VWS ≥ 9.7×10⁻³ s⁻¹\n",
    "UMBRAL_VWS = 9.7e-3  # s^-1\n",
    "\n",
    "vws_freq_models = []  # aquí guardaremos las frecuencias (%)\n",
    "\n",
    "for modelo, VWS in zip(modelos, VWS_daily_all_models):\n",
    "\n",
    "    print(f\"\\n Calculando frecuencia VWS ≥ {UMBRAL_VWS:.2e} s⁻¹ para {modelo}...\")\n",
    "\n",
    "    # VWS ya tiene coordenadas (time, lat, lon[, modelo])\n",
    "    # Crear máscara booleana (True si VWS ≥ umbral)\n",
    "    exceed_vws = VWS >= UMBRAL_VWS\n",
    "\n",
    "    # Calcular el porcentaje de días (respecto al total de días válidos)\n",
    "    freq_vws = exceed_vws.mean(dim=\"time\", skipna=True) * 100\n",
    "\n",
    "    # Añadir dimensión 'modelo' (solo si no existe)\n",
    "    if \"modelo\" not in freq_vws.dims:\n",
    "        freq_vws = freq_vws.expand_dims(modelo=[modelo])\n",
    "    else:\n",
    "        freq_vws = freq_vws.assign_coords(modelo=[modelo])\n",
    "\n",
    "    freq_vws.name = \"VWS_freq\"\n",
    "    freq_vws.attrs[\"units\"] = \"%\"\n",
    "    freq_vws.attrs[\"description\"] = (\n",
    "        f\"Porcentaje de días (2030–2100) con VWS ≥ {UMBRAL_VWS:.2e} s⁻¹ \"\n",
    "        \"(250–500 hPa, DJF)\"\n",
    "    )\n",
    "\n",
    "    # diagnóstico rápido\n",
    "    print(f\"   Máx frecuencia VWS para {modelo}: {float(freq_vws.max()):.3f} %\")\n",
    "\n",
    "    vws_freq_models.append(freq_vws)\n",
    "\n",
    "# GUARDAR EL NUEVO ARCHIVO NETCDF\n",
    "import gc  # Asegurar que está importado\n",
    "\n",
    "base_out_modelos = os.path.join(BASE_RESULTADOS, escenario, \"CAT index\", \"por_modelo\")\n",
    "os.makedirs(base_out_modelos, exist_ok=True)\n",
    "\n",
    "if len(ti1_freq_models) > 0:\n",
    "    print(\"\\n Guardando porcentaje de días con TI₁ > umbral por modelo...\")\n",
    "    ti1_freq_concat = xr.concat(ti1_freq_models, dim=\"modelo\")\n",
    "    ti1_freq_concat.load()\n",
    "    ti1_freq_concat.name = \"TI1_freq\"\n",
    "    ti1_freq_concat.attrs[\"units\"] = \"%\"\n",
    "    ti1_freq_concat.attrs[\"description\"] = (\n",
    "        f\"Frecuencia (2030–2100) de días con TI₁ > {UMBRAL_TI1:.1e} s⁻² \"\n",
    "        \"(250–500 hPa, DJF) por modelo\"\n",
    "    )\n",
    "\n",
    "    freq_path = os.path.join(base_out_modelos, f\"TI1_frecuencia_por_modelo_{escenario}_{UMBRAL_TI1}.nc\")\n",
    "    ti1_freq_concat.to_netcdf(freq_path, engine=\"h5netcdf\")\n",
    "    print(f\"✔ Guardado: {freq_path}\")\n",
    "\n",
    "    ti1_freq_concat.close()\n",
    "    gc.collect()\n",
    "\n",
    "# GUARDAR PORCENTAJE DE DÍAS CON VWS ≥ UMBRAL\n",
    "\n",
    "if len(vws_freq_models) > 0:\n",
    "    print(\"\\n Guardando porcentaje de días con VWS ≥ umbral por modelo...\")\n",
    "    vws_freq_concat = xr.concat(vws_freq_models, dim=\"modelo\")\n",
    "    vws_freq_concat.load()\n",
    "    vws_freq_concat.name = \"VWS_freq\"\n",
    "    vws_freq_concat.attrs[\"units\"] = \"%\"\n",
    "    vws_freq_concat.attrs[\"description\"] = (\n",
    "        f\"Frecuencia (2030–2100) de días con VWS ≥ {UMBRAL_VWS:.2e} s⁻¹ \"\n",
    "        \"(250–500 hPa, DJF) por modelo\"\n",
    "    )\n",
    "\n",
    "    vws_freq_path = os.path.join(\n",
    "        base_out_modelos,\n",
    "        f\"VWS_frecuencia_por_modelo_{escenario}_{UMBRAL_VWS}.nc\"\n",
    "    )\n",
    "    vws_freq_concat.to_netcdf(vws_freq_path, engine=\"h5netcdf\")\n",
    "    print(f\"✔ Guardado: {vws_freq_path}\")\n",
    "\n",
    "    vws_freq_concat.close()\n",
    "    gc.collect()\n",
    "\n",
    "# GUARDAR RESULTADOS POR MODELO (TI₁ y VWS, 250–500 hPa)\n",
    "\n",
    "# TI₁: Tendencias\n",
    "if len(ti1_slopes) > 0:\n",
    "    print(\"\\n Guardando tendencias de TI₁ por modelo...\")\n",
    "    ti1_tend_concat = xr.concat(ti1_slopes, dim=\"modelo\")\n",
    "    ti1_tend_concat.load()  # Carga completa en memoria (libera archivos fuente)\n",
    "    ti1_tend_concat.name = \"TI1_tendencia\"\n",
    "    ti1_tend_concat.attrs[\"units\"] = \"s^-2 por década\"\n",
    "    ti1_tend_concat.attrs[\"description\"] = \"Tendencia de TI₁ (250–500 hPa, DJF 2030–2100) por modelo\"\n",
    "    ti1_tend_path = os.path.join(base_out_modelos, f\"TI1_tendencias_por_modelo_{escenario}.nc\")\n",
    "    ti1_tend_concat.to_netcdf(ti1_tend_path, engine=\"h5netcdf\")\n",
    "    print(f\" Guardado: {ti1_tend_path}\")\n",
    "    ti1_tend_concat.close()\n",
    "    gc.collect()\n",
    "\n",
    "# TI₁: Medias climatológicas\n",
    "if len(ti1_mean_models) > 0:\n",
    "    print(\"\\n Guardando medias climatológicas de TI₁ por modelo...\")\n",
    "    ti1_mean_concat = xr.concat(ti1_mean_models, dim=\"modelo\")\n",
    "    ti1_mean_concat.load()\n",
    "    ti1_mean_concat.name = \"TI1_media\"\n",
    "    ti1_mean_concat.attrs[\"units\"] = \"s^-2\"\n",
    "    ti1_mean_concat.attrs[\"description\"] = \"Media climatológica DJF de TI₁ (250–500 hPa, 2030–2100) por modelo\"\n",
    "    ti1_mean_path = os.path.join(base_out_modelos, f\"TI1_media_por_modelo_{escenario}.nc\")\n",
    "    ti1_mean_concat.to_netcdf(ti1_mean_path, engine=\"h5netcdf\")\n",
    "    print(f\" Guardado: {ti1_mean_path}\")\n",
    "    ti1_mean_concat.close()\n",
    "    gc.collect()\n",
    "\n",
    "# VWS: Tendencias\n",
    "if len(shear_slopes) > 0:\n",
    "    print(\"\\n Guardando tendencias de VWS por modelo...\")\n",
    "    VWS_tend_concat = xr.concat(shear_slopes, dim=\"modelo\")\n",
    "    VWS_tend_concat.load()\n",
    "    VWS_tend_concat.name = \"VWS_tendencia\"\n",
    "    VWS_tend_concat.attrs[\"units\"] = \"s^-1 por década\"\n",
    "    VWS_tend_concat.attrs[\"description\"] = \"Tendencia de la cizalladura vertical (VWS) en 250–500 hPa (DJF 2030–2100) por modelo\"\n",
    "    VWS_tend_path = os.path.join(base_out_modelos, f\"VWS_tendencias_por_modelo_{escenario}.nc\")\n",
    "    VWS_tend_concat.to_netcdf(VWS_tend_path, engine=\"h5netcdf\")\n",
    "    print(f\" Guardado: {VWS_tend_path}\")\n",
    "    VWS_tend_concat.close()\n",
    "    gc.collect()\n",
    "\n",
    "# VWS: Medias climatológicas\n",
    "if len(VWS_mean_models) > 0:\n",
    "    print(\"\\n Guardando medias climatológicas de VWS por modelo...\")\n",
    "    VWS_mean_concat = xr.concat(VWS_mean_models, dim=\"modelo\")\n",
    "    VWS_mean_concat.load()\n",
    "    VWS_mean_concat.name = \"VWS_media\"\n",
    "    VWS_mean_concat.attrs[\"units\"] = \"s^-1\"\n",
    "    VWS_mean_concat.attrs[\"description\"] = \"Media climatológica DJF de la cizalladura vertical (VWS) en 250–500 hPa (2030–2100) por modelo\"\n",
    "    VWS_mean_path = os.path.join(base_out_modelos, f\"VWS_media_por_modelo_{escenario}.nc\")\n",
    "    VWS_mean_concat.to_netcdf(VWS_mean_path, engine=\"h5netcdf\")\n",
    "    print(f\" Guardado: {VWS_mean_path}\")\n",
    "    VWS_mean_concat.close()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n Comprobando archivos guardados...\")\n",
    "for fname in [\n",
    "    f\"TI1_tendencias_por_modelo_{escenario}.nc\",\n",
    "    f\"TI1_media_por_modelo_{escenario}.nc\",\n",
    "    f\"VWS_tendencias_por_modelo_{escenario}.nc\",\n",
    "    f\"VWS_media_por_modelo_{escenario}.nc\"\n",
    "]:\n",
    "    fpath = os.path.join(base_out_modelos, fname)\n",
    "    if os.path.exists(fpath):\n",
    "        ds = xr.open_dataset(fpath)\n",
    "        print(f\" {fname}: {list(ds.dims.keys())} | modelos = {len(ds.modelo)}\")\n",
    "        ds.close()\n",
    "    else:\n",
    "        print(f\" No se encontró {fname}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
